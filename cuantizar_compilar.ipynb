{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar librerias\n"
      ],
      "metadata": {
        "id": "x1clefq1ArL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPtpBuQCVYfa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.api._v2.keras.layers import Normalization\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLVefZuboRrF",
        "outputId": "be401772-d0c2-4309-f04b-bf0ebf7fdcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K4SjO-o2sOW"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "seed=42\n",
        "IMAGE_SIZE = 224\n",
        "NUM_CLASSES = 4\n",
        "DATA_DIR = \"/content/drive/MyDrive/MIoT/TFM\"\n",
        "NUM_TRAIN_IMAGES= 323\n",
        "#376\n",
        "#323\n",
        "NUM_TEST_IMAGES = 79\n",
        "#96\n",
        "#79"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generar los datos"
      ],
      "metadata": {
        "id": "S__XutxLA094"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dadv_dH83VjJ",
        "outputId": "01e4af92-208a-4b7b-d7fa-c540bf5364ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_ParallelMapDataset element_spec=(TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(224, 224, 1), dtype=tf.float32, name=None))>\n",
            "Val Dataset: <_ParallelMapDataset element_spec=(TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(224, 224, 1), dtype=tf.float32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR, \"train_img/*\")))[:NUM_TRAIN_IMAGES]\n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR, \"train_mask_ids/*\")))[:NUM_TRAIN_IMAGES]\n",
        "\n",
        "val_images = sorted(glob(os.path.join(DATA_DIR, \"test_img/*\")))[:NUM_TEST_IMAGES]\n",
        "val_masks = sorted(glob(os.path.join(DATA_DIR, \"test_mask_ids/*\")))[:NUM_TEST_IMAGES]\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image.set_shape([None, None, 1])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "        image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_images, train_masks)\n",
        "val_dataset = data_generator(val_images, val_masks)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aumentar los datos"
      ],
      "metadata": {
        "id": "jRsBNEmgBFKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN9uIq-03a1y"
      },
      "outputs": [],
      "source": [
        "class Augment(tf.keras.layers.Layer):\n",
        "    def __init__(self, seed=42):\n",
        "        super().__init__()\n",
        "\n",
        "        self.augment_inputs = [\n",
        "            tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed),\n",
        "            tf.keras.layers.RandomRotation(factor=0.25, fill_mode=\"wrap\", seed=seed),\n",
        "            tf.keras.layers.RandomTranslation(height_factor=0.25, width_factor=0.25, fill_mode=\"wrap\", seed=seed),\n",
        "            tf.keras.layers.RandomZoom(height_factor=(-0.3, -0.2), fill_mode=\"wrap\", seed=seed)\n",
        "        ]\n",
        "\n",
        "        self.augment_labels = [\n",
        "            tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed),\n",
        "            tf.keras.layers.RandomRotation(factor=0.25, fill_mode=\"wrap\", seed=seed),\n",
        "            tf.keras.layers.RandomTranslation(height_factor=0.25, width_factor=0.25, fill_mode=\"wrap\", seed=seed),\n",
        "            tf.keras.layers.RandomZoom(height_factor=(-0.3, -0.2), fill_mode=\"wrap\", seed=seed)\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs, labels):\n",
        "        augmented_inputs = [inputs]\n",
        "        augmented_labels = [labels]\n",
        "\n",
        "        for augment_input in self.augment_inputs:\n",
        "            augmented_inputs.append(augment_input(inputs))\n",
        "\n",
        "        for augment_label in self.augment_labels:\n",
        "            augmented_labels.append(augment_label(labels))\n",
        "\n",
        "        augmented_inputs = tf.stack(augmented_inputs)\n",
        "        augmented_labels = tf.stack(augmented_labels)\n",
        "\n",
        "        return augmented_inputs, augmented_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear 'batch'"
      ],
      "metadata": {
        "id": "WxmIDfLaBLJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyZkzoFL3hGe"
      },
      "outputs": [],
      "source": [
        "train_batches = (\n",
        "    train_dataset\n",
        "    .map(Augment())\n",
        "    .unbatch()\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_batches = val_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPMbT-d83jOr",
        "outputId": "d312fc63-ae20-4e74-cf84-4eecf085a60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None))>\n",
            "Val Dataset: <_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Dataset:\", train_batches)\n",
        "print(\"Val Dataset:\", test_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuantizar el modelo"
      ],
      "metadata": {
        "id": "Qu2JerYSCL93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8AV2tB3lPFI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cargar el modelo\n",
        "load_h5= r'C:\\Users\\Paola\\Documents\\master\\TFM\\seg_quanti\\modelo_finales_h5\\with_MobileNetV3Large_dt1_decay.h5'\n",
        "\n",
        "model = tf.keras.models.load_model(load_h5)\n",
        "\n",
        "\n",
        "# Load the dataset with data augmentations\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    imgs, batch_labels = next(iter(train_batches))\n",
        "    for i in range(len(imgs)):\n",
        "        img= imgs[i]\n",
        "        img = tf.cast(img, tf.float32)\n",
        "        img = tf.expand_dims(img, 0) # ---> obtenemos (1,224,223,3)\n",
        "        print('SHAPE',img.shape)\n",
        "        print('mira',i)\n",
        "        yield [img]\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "print('Comenzo')\n",
        "tflite_model = converter.convert()\n",
        "print('Acabo')\n",
        "\n",
        "# Save the quantized model\n",
        "save_tflite =r\"...\\modelos_finales_tflite\\with_MobileNetV3Large_dt1_decay_int8_quant.tflite\"\n",
        "with open(save_tflite, 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6zy53qYbev"
      },
      "source": [
        "# Evaluación del modelo sin cuantizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRJPTzvy8tES"
      },
      "outputs": [],
      "source": [
        "def infer(model, image_tensor):\n",
        "    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n",
        "    predictions = np.squeeze(predictions)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_r0XAS85Hl",
        "outputId": "2fea8104-0110-4b93-87a4-cb2d47013227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 446ms/step\n",
            "1/1 [==============================] - 0s 469ms/step\n",
            "1/1 [==============================] - 0s 453ms/step\n",
            "1/1 [==============================] - 0s 455ms/step\n",
            "1/1 [==============================] - 0s 450ms/step\n",
            "1/1 [==============================] - 0s 453ms/step\n",
            "1/1 [==============================] - 1s 735ms/step\n",
            "1/1 [==============================] - 1s 744ms/step\n",
            "1/1 [==============================] - 1s 752ms/step\n",
            "1/1 [==============================] - 0s 457ms/step\n",
            "1/1 [==============================] - 0s 451ms/step\n",
            "1/1 [==============================] - 0s 462ms/step\n",
            "1/1 [==============================] - 0s 443ms/step\n",
            "1/1 [==============================] - 0s 455ms/step\n",
            "1/1 [==============================] - 0s 441ms/step\n",
            "1/1 [==============================] - 0s 441ms/step\n",
            "1/1 [==============================] - 0s 459ms/step\n",
            "1/1 [==============================] - 0s 433ms/step\n",
            "1/1 [==============================] - 0s 445ms/step\n",
            "1/1 [==============================] - 0s 457ms/step\n",
            "1/1 [==============================] - 0s 445ms/step\n",
            "1/1 [==============================] - 0s 467ms/step\n",
            "1/1 [==============================] - 0s 447ms/step\n",
            "1/1 [==============================] - 1s 587ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 440ms/step\n",
            "1/1 [==============================] - 0s 420ms/step\n",
            "1/1 [==============================] - 0s 451ms/step\n",
            "1/1 [==============================] - 0s 435ms/step\n",
            "1/1 [==============================] - 0s 449ms/step\n",
            "Raw model mIOU: 87.214%\n",
            "Raw model Agua IOU: 76.034%\n",
            "Raw model Cyano IOU: 91.092%\n"
          ]
        }
      ],
      "source": [
        "new_model = tf.keras.models.load_model('/content/drive/MyDrive/MIoT/TFM/Modelos_finales/With_DenseNet201_dt1_decay_save')\n",
        "prediction=[]\n",
        "truth = []\n",
        "batch_images, batch_labels = next(iter(test_batches))\n",
        "for image_file, ground_truth_file in zip(batch_images, batch_labels):\n",
        "  #image_tensor = read_image(image_file)\n",
        "  #ground_truth_tensor = read_image(ground_truth_file, mask=True)\n",
        "  image_tensor= image_file\n",
        "  ground_truth_tensor = ground_truth_file\n",
        "  prediction_mask = infer(image_tensor=image_tensor, model=new_model)\n",
        "  ground_truth_mask =  np.squeeze(ground_truth_tensor).astype(np.uint8)\n",
        "  prediction.append(prediction_mask)\n",
        "  truth.append(ground_truth_mask)\n",
        "\n",
        "prediction_stack = tf.stack(prediction)\n",
        "truth_stack = tf.stack(truth)\n",
        "\n",
        "keras_accuracy = [tf.keras.metrics.MeanIoU(num_classes=4, sparse_y_true = True, sparse_y_pred = True),\n",
        "                  tf.keras.metrics.IoU(num_classes=4, target_class_ids=[1], sparse_y_true = True, sparse_y_pred = True),\n",
        "                  tf.keras.metrics.IoU(num_classes=4, target_class_ids=[2], sparse_y_true = True, sparse_y_pred = True)]\n",
        "keras_accuracy[0](prediction_stack, truth_stack)\n",
        "keras_accuracy[1](prediction_stack, truth_stack)\n",
        "keras_accuracy[2](prediction_stack, truth_stack)\n",
        "\n",
        "\n",
        "print(\"Raw model mIOU: {:.3%}\".format(keras_accuracy[0].result()))\n",
        "print(\"Raw model Agua IOU: {:.3%}\".format(keras_accuracy[1].result()))\n",
        "print(\"Raw model Cyano IOU: {:.3%}\".format(keras_accuracy[2].result()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-P9GiKlYmac"
      },
      "source": [
        "# Evaluación del modelo cuantizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xonLXOxm_Lm0",
        "outputId": "a0155080-86fe-421a-c4ff-ca9db02ae999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quant TF Lite mIOU: 84.152%\n",
            "Quant TF Lite Agua IOU: 69.521%\n",
            "Quant TF Lite Cyano IOU: 87.455%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_images, batch_labels = next(iter(test_batches))\n",
        "def set_input_tensor(interpreter, input):\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  tensor_index = input_details['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  # Inputs for the TFLite model must be uint8, so we quantize our input data.\n",
        "  # NOTE: This step is necessary only because we're receiving input data from\n",
        "  # ImageDataGenerator, which rescaled all image data to float [0,1]. When using\n",
        "  # bitmap inputs, they're already uint8 [0,255] so this can be replaced with:\n",
        "  #input_tensor[:, :] = input\n",
        "  scale, zero_point = input_details['quantization']\n",
        "  input_tensor[:, :] = np.int8(input / scale + zero_point)\n",
        "\n",
        "def classify_image(interpreter, input):\n",
        "  set_input_tensor(interpreter, input)\n",
        "  interpreter.invoke()\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  output = interpreter.get_tensor(output_details['index'])\n",
        "  # Outputs from the TFLite model are uint8, so we dequantize the results:\n",
        "  scale, zero_point = output_details['quantization']\n",
        "  output = scale * (output - zero_point)\n",
        "  output = np.squeeze(output)\n",
        "  top_1 = np.argmax(output,axis=2)\n",
        "  return top_1\n",
        "\n",
        "interpreter = tf.lite.Interpreter('/content/drive/MyDrive/MIoT/TFM/Modelos_tflite/with_ConvNeXtSmall_dt1_decay_int8_agua_t_aug_quant.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Collect all inference predictions in a list\n",
        "batch_prediction = []\n",
        "batch_truth = np.squeeze(batch_labels)\n",
        "\n",
        "for i in range(len(batch_images)):\n",
        "  prediction = classify_image(interpreter, batch_images[i])\n",
        "  batch_prediction.append(prediction)\n",
        "\n",
        "batch_prediction_stack= tf.stack(batch_prediction)\n",
        "\n",
        "# Compare all predictions to the ground truth\n",
        "tflite_accuracy = [tf.keras.metrics.MeanIoU(num_classes=4, sparse_y_true = True, sparse_y_pred = True),\n",
        "                  tf.keras.metrics.IoU(num_classes=4, target_class_ids=[1], sparse_y_true = True, sparse_y_pred = True),\n",
        "                   tf.keras.metrics.IoU(num_classes=4, target_class_ids=[2], sparse_y_true = True, sparse_y_pred = True)]\n",
        "tflite_accuracy[0](batch_prediction, batch_truth)\n",
        "tflite_accuracy[1](batch_prediction, batch_truth)\n",
        "tflite_accuracy[2](batch_prediction, batch_truth)\n",
        "\n",
        "print(\"Quant TF Lite mIOU: {:.3%}\".format(tflite_accuracy[0].result()))\n",
        "print(\"Quant TF Lite Agua IOU: {:.3%}\".format(tflite_accuracy[1].result()))\n",
        "print(\"Quant TF Lite Cyano IOU: {:.3%}\".format(tflite_accuracy[2].result()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compilación .tflite para TPU"
      ],
      "metadata": {
        "id": "sJWy67R1AAaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP-1okA1GRxm",
        "outputId": "7ab4d5a8-5870-4778-93dc-b1c755951250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2659  100  2659    0     0  91689      0 --:--:-- --:--:-- --:--:-- 91689\n",
            "OK\n",
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:3 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6,332 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:9 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 Packages [2,317 B]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,776 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,255 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,354 kB]\n",
            "Hit:16 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 7,733 kB in 1s (5,805 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  edgetpu-compiler\n",
            "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 7,913 kB of archives.\n",
            "After this operation, 31.2 MB of additional disk space will be used.\n",
            "Get:1 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu-compiler amd64 16.0 [7,913 kB]\n",
            "Fetched 7,913 kB in 0s (20.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package edgetpu-compiler.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../edgetpu-compiler_16.0_amd64.deb ...\n",
            "Unpacking edgetpu-compiler (16.0) ...\n",
            "Setting up edgetpu-compiler (16.0) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbc4AZyI_W9_",
        "outputId": "ba20b4c4-af54-4ca5-c2eb-7bff90b7d1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge TPU Compiler version 16.0.384591198\n",
            "ERROR: Didn't find op for builtin opcode 'CONV_2D' version '6'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\n",
            "\n",
            "ERROR: Registration failed.\n",
            "\n",
            "Invalid model: /content/drive/MyDrive/MIoT/TFM/Modelos_tflite/with_ConvNeXtSmall_dt1_decay_int8_quant.tflite\n",
            "Model could not be parsed\n"
          ]
        }
      ],
      "source": [
        "! edgetpu_compiler /content/drive/MyDrive/MIoT/TFM/Modelos_tflite/with_ConvNeXtSmall_dt1_decay_int8_quant.tflite"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}