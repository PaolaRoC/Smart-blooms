# -*- coding: utf-8 -*-
"""inferir_trt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lJECCcVSPdiJlj6bj_e5LHBJbYyGb5NS
"""

import argparse
from ast import arg
import tensorrt as trt
from cuda import cudart

import tensorflow as tf
import numpy as np
import time
from tqdm import tqdm
import csv
import os
import sys
sys.path.insert(1, os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
import common

print(trt.__version__)


import os
from glob import glob

seed=42
IMAGE_SIZE = 224
NUM_CLASSES = 4



def read_image(image_path, mask=False):
    image = tf.io.read_file(image_path)

    if mask:
        image = tf.image.decode_png(image, channels=1)
        image.set_shape([None, None, 1])
        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
    else:
        image = tf.image.decode_png(image, channels=3)
        image.set_shape([None, None, 3])
        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
        image = tf.keras.applications.resnet50.preprocess_input(image)
    return image


def load_data(image_list, mask_list):
    image = read_image(image_list)
    mask = read_image(mask_list, mask=True)
    return image, mask


def data_generator(image_list, mask_list):

    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))
    dataset = dataset.map(load_data,num_parallel_calls=tf.data.AUTOTUNE)
    return dataset

#######################################################################################
def archivo_existe(archivo_csv):
    if os.path.exists(archivo_csv):
        print(f"El archivo CSV '{archivo_csv}' existe.")
        encabezado = tiene_encabezado(archivo_csv)
        print('Tiene encabesado: ', encabezado)
        return encabezado

    else:
        print(f"El archivo CSV '{archivo_csv}' no existe.")
        return False


def tiene_encabezado(archivo_csv):
    with open(archivo_csv, 'r', newline='') as file:
        reader = csv.reader(file)
        primera_linea = next(reader, None)

        if primera_linea:
            return True
        else:
            return False 

##############################################################

class TensorRTInfer:
    """
    Implements inference for the Model TensorRT engine.
    """

    def __init__(self, engine_path):
        """
        :param engine_path: The path to the serialized engine to load from disk.
        """
        # Load TRT engine
        self.logger = trt.Logger(trt.Logger.ERROR)
        trt.init_libnvinfer_plugins(self.logger, namespace="")
        with open(engine_path, "rb") as f, trt.Runtime(self.logger) as runtime:
            assert runtime
            self.engine = runtime.deserialize_cuda_engine(f.read())
        assert self.engine
        self.context = self.engine.create_execution_context()
        assert self.context

        # Setup I/O bindings
        self.inputs = []
        self.outputs = []
        self.allocations = []
        for i in range(self.engine.num_bindings):
            is_input = False
            if self.engine.binding_is_input(i):
                is_input = True
            name = self.engine.get_binding_name(i)
            dtype = self.engine.get_binding_dtype(i)
            shape = self.engine.get_binding_shape(i)
            if is_input:
                self.batch_size = shape[0]
            size = np.dtype(trt.nptype(dtype)).itemsize
            for s in shape:
                size *= s
            allocation = common.cuda_call(cudart.cudaMalloc(size))
            binding = {
                'index': i,
                'name': name,
                'dtype': np.dtype(trt.nptype(dtype)),
                'shape': list(shape),
                'allocation': allocation,
            }
            self.allocations.append(allocation)
            if self.engine.binding_is_input(i):
                self.inputs.append(binding)
            else:
                self.outputs.append(binding)

        assert self.batch_size > 0
        assert len(self.inputs) > 0
        assert len(self.outputs) > 0
        assert len(self.allocations) > 0

    def input_spec(self):
        """
        Get the specs for the input tensor of the network. Useful to prepare memory allocations.
        :return: Two items, the shape of the input tensor and its (numpy) datatype.
        """
        return self.inputs[0]['shape'], self.inputs[0]['dtype']

    def output_spec(self):
        """
        Get the specs for the output tensor of the network. Useful to prepare memory allocations.
        :return: Two items, the shape of the output tensor and its (numpy) datatype.
        """
        return self.outputs[0]["shape"], self.outputs[0]["dtype"]

    def infer(self, batch,N_W_RUN,N_RUN,test_batches_label,BATCH_SIZE):
        N_warmup_run = N_W_RUN
        N_run = N_RUN
        elapsed_time = []
        shape_out, dtype_out = self.output_spec() 
        n_labels= 4
        # Prepare the output data
        output = np.zeros((test_batches_label.shape[0],224,224,n_labels), dtype_out) 
        #output = np.zeros(*self.output_spec()) # esto me crea una array del shape ((32, 224, 224, 4)) porque el modelo fue creado con ese tama√±o en onnx. 

        # Process I/O and execute the network
        common.memcpy_host_to_device(self.inputs[0]['allocation'], np.ascontiguousarray(batch))


        for _ in range(N_warmup_run):
            self.context.execute_v2(self.allocations)

        for _ in tqdm(range(N_run)):
            start_time = time.time()

            self.context.execute_v2(self.allocations)
        
            end_time = time.time()
            elapsed_time = np.append(elapsed_time, end_time - start_time)

        common.memcpy_device_to_host(output, self.outputs[0]["allocation"])
       
        # Process the results
        IMG_S = N_run * BATCH_SIZE / elapsed_time.sum()
        predictions = np.argmax(output, axis=3)    
        test_batches_label = np.squeeze(test_batches_label)
     
        keras_accuracy = [tf.keras.metrics.MeanIoU(num_classes=4, sparse_y_true = True, sparse_y_pred = True),
                    tf.keras.metrics.IoU(num_classes=4, target_class_ids=[1], sparse_y_true = True, sparse_y_pred = True),
                    tf.keras.metrics.IoU(num_classes=4, target_class_ids=[2], sparse_y_true = True, sparse_y_pred = True)]
        
        keras_accuracy[0](predictions, test_batches_label)
        keras_accuracy[1](predictions, test_batches_label)
        keras_accuracy[2](predictions, test_batches_label)

        return [round((keras_accuracy[0].result().numpy()*100),3), round((keras_accuracy[1].result().numpy()*100),3), round((keras_accuracy[2].result().numpy()*100),3), BATCH_SIZE, round((min(elapsed_time)*1000),1), round((max(elapsed_time)*1000),1), round((elapsed_time.mean()*1000),1), round(IMG_S)]

    
        

############################################################

def main():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter)
  parser.add_argument(
      '--model', help='File path of model.', required=True)
  parser.add_argument(
      '--data_dir', help='Path of data.', required=True)
  parser.add_argument(
     '--batch_size', type=int, help='Size of batch', required=True)
  parser.add_argument(
     '--N_warmup_run', type=int, help= 'Number of warmup run', required=True)
  parser.add_argument(
     '--N_run', type=int,  help='Number run', required=True)
  parser.add_argument(
      '--name_csv', help='Name of csv file.', required=True)

  args = parser.parse_args()

  
  BATCH_SIZE = args.batch_size
  N_W_RUN = args.N_warmup_run
  N_RUN = args.N_run 
  print(BATCH_SIZE)
  DATA_DIR = args.data_dir
  archivo_csv = args.name_csv
  NUM_TEST_IMAGES = 79

      
  val_images = sorted(glob(os.path.join(DATA_DIR, "test_img/*")))[:NUM_TEST_IMAGES]
  val_masks = sorted(glob(os.path.join(DATA_DIR, "test_mask_ids/*")))[:NUM_TEST_IMAGES]

  
  val_dataset = data_generator(val_images, val_masks)

  test_batches = (
    val_dataset
    .batch(BATCH_SIZE)
    .prefetch(buffer_size=tf.data.AUTOTUNE))
  

  field = ["Modelo", "Placa", "Precision", "mIoU", "AguaIoU", "CyanoIoU", "Batch", "Tiempo min", "Tiempo max", "Tiempo mean", "Img/s"]
  
  info_takes= []
  n_len_batch = round(len(val_images)/BATCH_SIZE)
  print(n_len_batch)

  trt_infer = TensorRTInfer(args.model)
  
  for images, masks in test_batches.take(n_len_batch):
    info_takes.append(trt_infer.infer(images,N_W_RUN,N_RUN, masks,BATCH_SIZE))



  matriz_info = np.array(info_takes)

  nb_precision = args.model.split("_")[-2]
  nb_placa = args.model.split("_")[-1]
  nb_modelo = args.model.split("_")[2]
  verf_archivo = archivo_existe(archivo_csv)
  if verf_archivo == True:
    with open(archivo_csv,'a') as f1:
        writer= csv.writer(f1, delimiter='\t',lineterminator='\n',)
        writer.writerow([nb_modelo, nb_placa, nb_precision, round(np.mean(matriz_info[:,0]),3), round(np.mean(matriz_info[:,1]),3), round(np.mean(matriz_info[:,2]),3), BATCH_SIZE, round(np.mean(matriz_info[:,4]),1), round(np.mean(matriz_info[:,5]),1), round(np.mean(matriz_info[:,6]),1), round(np.mean(matriz_info[:,7]))])
  else:
    with open(archivo_csv, 'w') as f1:
        writer= csv.writer(f1, delimiter='\t',lineterminator='\n',)
        writer.writerow(field)
        writer.writerow([nb_modelo, nb_placa, nb_precision, round(np.mean(matriz_info[:,0]),3), round(np.mean(matriz_info[:,1]),3), round(np.mean(matriz_info[:,2]),3), BATCH_SIZE, round(np.mean(matriz_info[:,4]),1), round(np.mean(matriz_info[:,5]),1), round(np.mean(matriz_info[:,6]),1), round(np.mean(matriz_info[:,7]))])



if __name__ == '__main__':
  main()